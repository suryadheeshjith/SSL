{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RAbz2m_s6VSD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEnRulK57d-m",
    "outputId": "6283c294-1e4a-4c06-a705-e36653f7030f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B8eslUNWZfI3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "\n",
    "# These numbers are mean and std values for channels of natural images. \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Inverse transformation: needed for plotting.\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "motion_transform_train = transforms.Compose([normalize])\n",
    "\n",
    "content_transform_train = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(size=(160,240)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.GaussianBlur(7, sigma=(0.1, 1.0)),\n",
    "                                    normalize,\n",
    "                                ])\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, data_dir, num_frames=22, motion_transform=motion_transform_train):\n",
    "        self.data_dir = data_dir\n",
    "        self.motion_transform = motion_transform\n",
    "        self.num_frames = num_frames\n",
    "        self.video_list = []\n",
    "\n",
    "        self.count = 0\n",
    "        for vid_dir in os.listdir(self.data_dir):\n",
    "            self.video_list.append(self.data_dir +\"/\"+vid_dir)\n",
    "            self.count +=1\n",
    "            \n",
    "            if self.count == 2500:\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        video_dir = self.video_list[idx]\n",
    "        frame_list = []\n",
    "        \n",
    "        for i in range(self.num_frames):\n",
    "            image = read_image(video_dir + \"/\" + \"image_\"+str(i)+\".png\")\n",
    "            image = image/255.0\n",
    "            \n",
    "            if self.motion_transform:\n",
    "                image = self.motion_transform(image)\n",
    "            \n",
    "            frame_list.append(image)  \n",
    "\n",
    "        return frame_list\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, data_dir, num_frames=22, motion_transform=motion_transform_train):\n",
    "        self.data_dir = data_dir\n",
    "        self.motion_transform = motion_transform\n",
    "        self.num_frames = num_frames\n",
    "        self.video_list = []\n",
    "\n",
    "        self.count = 0\n",
    "        for vid_dir in os.listdir(self.data_dir):\n",
    "            self.video_list.append(self.data_dir +\"/\"+vid_dir)\n",
    "            self.count +=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        video_dir = self.video_list[idx]\n",
    "        frame_list = []\n",
    "        \n",
    "        for i in range(self.num_frames):\n",
    "            image = read_image(video_dir + \"/\" + \"image_\"+str(i)+\".png\")\n",
    "            image = image/255.0\n",
    "            \n",
    "            if self.motion_transform:\n",
    "                image = self.motion_transform(image)\n",
    "            \n",
    "            frame_list.append(image)\n",
    "        \n",
    "        label = -1\n",
    "        if os.path.isfile(video_dir+\"/mask.npy\"):\n",
    "            try:\n",
    "                label = np.load(video_dir+\"/mask.npy\")\n",
    "            except:\n",
    "                return None, None\n",
    "        \n",
    "\n",
    "        return frame_list, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unlabeled_data = UnlabeledDataset(\"/dataset/dataset/unlabeled\")\n",
    "labeled_data = LabeledDataset(\"/dataset/dataset/train\")\n",
    "val_data = LabeledDataset(\"/dataset/dataset/val\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(unlabeled_data, batch_size=3, shuffle=True)\n",
    "downstream_dataloader = DataLoader(labeled_data, batch_size=3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "q5IGtUuiZgKB",
    "outputId": "eb8327e5-1e51-4b82-fa83-0f94cf22cf38",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.rcParams['figure.dpi'] = 100 # change dpi to make plots bigger\n",
    "\n",
    "# def show_normalized_image(img, title=None):\n",
    "#     plt.imshow(unnormalize(img).detach().cpu().permute(1, 2, 0).numpy())\n",
    "#     plt.title(title)\n",
    "#     plt.axis('off')\n",
    "\n",
    "# show_normalized_image(unlabeled_data[10][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated MC Jepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_features, out_features, kernel_size=3, stride=1, padding=1, dilation=1):   \n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=kernel_size, stride=stride, \n",
    "                        padding=padding, dilation=dilation),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "            nn.LeakyReLU(0.1))\n",
    "\n",
    "def flow_cnn(in_features):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_features, 2, kernel_size=3, stride=1, \n",
    "                        padding=1),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.LeakyReLU(0.1))\n",
    "\n",
    "\n",
    "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding)\n",
    "\n",
    "\n",
    "class FEA(nn.Module):\n",
    "    def __init__ (self, in_features):\n",
    "        super().__init__()\n",
    "        self.flow_predictor = flow_cnn(in_features)\n",
    "\n",
    "    def forward(self, X_tnext, X_hat_tnext):\n",
    "        correlation = self.corr4D(X_tnext, X_hat_tnext)\n",
    "        batch_size, out_channels, ht, wd = X_tnext.shape\n",
    "\n",
    "        mat_mul = torch.matmul(correlation, X_tnext.view(batch_size, out_channels, ht*wd, 1))\n",
    "        mat_mul = mat_mul.view(batch_size, out_channels, ht, wd)\n",
    "        x = self.flow_predictor(mat_mul)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def corr4D(X_tnext, X_hat_tnext):\n",
    "        batch, dim, ht, wd = X_tnext.shape\n",
    "        X_tnext = X_tnext.view(batch, dim, ht*wd)\n",
    "        X_hat_tnext = X_hat_tnext.view(batch, dim, ht*wd) \n",
    "\n",
    "        corr = torch.matmul(X_tnext.transpose(1,2), X_hat_tnext)\n",
    "        corr = corr.view(batch, 1, ht*wd, ht*wd)\n",
    "        corr = corr/torch.sqrt(torch.tensor(dim).float())\n",
    "        return corr\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def upsample(flow, scale = 2, mode='bilinear'):\n",
    "#         new_size = (scale * flow.shape[2], scale * flow.shape[3])\n",
    "#         return  scale * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)\n",
    "\n",
    "def warp(x, flo):\n",
    "    \"\"\"\n",
    "    warp an image/tensor (im2) back to im1, according to the optical flow\n",
    "    x: [B, C, H, W] (im2)\n",
    "    flo: [B, 2, H, W] flow\n",
    "    \"\"\"\n",
    "\n",
    "    B, C, H, W = x.size()\n",
    "    # mesh grid \n",
    "    xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n",
    "    yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n",
    "    xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n",
    "    yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n",
    "    grid = torch.cat((xx,yy),1).float().to(device)\n",
    "\n",
    "    if torch.is_tensor(flo): \n",
    "        vgrid = torch.autograd.Variable(grid) + flo\n",
    "    else:\n",
    "        vgrid = torch.autograd.Variable(grid)\n",
    "\n",
    "    # scale grid to [-1,1] \n",
    "    vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:].clone() / max(W-1,1)-1.0\n",
    "    vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:].clone() / max(H-1,1)-1.0\n",
    "\n",
    "    vgrid = vgrid.permute(0,2,3,1)        \n",
    "    output = nn.functional.grid_sample(x, vgrid)\n",
    "    mask = torch.autograd.Variable(torch.ones(x.size())).to(device)\n",
    "    mask = nn.functional.grid_sample(mask, vgrid)\n",
    "\n",
    "    mask[mask<0.9999] = 0\n",
    "    mask[mask>0] = 1\n",
    "    return output*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCJepa(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv(in_features, 4, 3, 2)\n",
    "        self.conv1a = conv(4, 4, 3)\n",
    "        self.conv1b = conv(4, 4, 3)\n",
    "        self.conv1c = conv(4, 4, 3)\n",
    "        \n",
    "        self.conv2 = conv(4, 8, 3, 2)\n",
    "        self.conv2a = conv(8, 8, 3)\n",
    "        self.conv2b = conv(8, 8, 3)\n",
    "        self.conv2c = conv(8, 8, 3)\n",
    "        \n",
    "        self.conv3 = conv(8, 12, 3, 2)\n",
    "        self.conv3a = conv(12, 12, 3)\n",
    "        self.conv3b = conv(12, 12, 3)\n",
    "        self.conv3c = conv(12, 12, 3)\n",
    "        \n",
    "        self.conv4 = conv(12, 16, 3, 2)\n",
    "        self.conv4a = conv(16, 16, 3)\n",
    "        self.conv4b = conv(16, 16, 3)\n",
    "        self.conv4c = conv(16, 16, 3)\n",
    "        \n",
    "        \n",
    "        self.fea4 = FEA(16)\n",
    "        self.deconv4 = deconv(2, 2)\n",
    "        \n",
    "        self.fea3 = FEA(12)\n",
    "        self.deconv3 = deconv(2, 2)\n",
    "        \n",
    "        self.fea2 = FEA(8)\n",
    "        self.deconv2 = deconv(2, 2)\n",
    "        \n",
    "        self.fea1 = FEA(4)\n",
    "        self.deconv1 = deconv(2, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, I_t, I_tnext): # , I_tcrop):\n",
    "        \n",
    "        X_t = None\n",
    "        X_tnext = None\n",
    "        X_hat_tnext = None\n",
    "        X_hat_t = None\n",
    "        f_t_tnext = None\n",
    "        f_tnext_t = []\n",
    "        \n",
    "        # Image t downsampling\n",
    "        I_t_x1 = self.conv1c(self.conv1b(self.conv1a(self.conv1(I_t))))\n",
    "        I_t_x2 = self.conv2c(self.conv2b(self.conv2a(self.conv2(I_t_x1))))\n",
    "        I_t_x3 = self.conv3c(self.conv3b(self.conv3a(self.conv3(I_t_x2))))\n",
    "        I_t_x4 = self.conv4c(self.conv4b(self.conv4a(self.conv4(I_t_x3))))\n",
    "        \n",
    "        X_t = [I_t_x1, I_t_x2, I_t_x3, I_t_x4]\n",
    "#         X_t = [I_t_x1, I_t_x2, I_t_x3]\n",
    "#         X_t = [I_t_x1, I_t_x2]\n",
    "        \n",
    "        # Image t+1 downsampling\n",
    "        I_tnext_x1 = self.conv1c(self.conv1b(self.conv1a(self.conv1(I_tnext))))\n",
    "        I_tnext_x2 = self.conv2c(self.conv2b(self.conv2a(self.conv2(I_tnext_x1))))\n",
    "        I_tnext_x3 = self.conv3c(self.conv3b(self.conv3a(self.conv3(I_tnext_x2))))\n",
    "        I_tnext_x4 = self.conv4c(self.conv4b(self.conv4a(self.conv4(I_tnext_x3))))\n",
    "        \n",
    "        X_tnext = [I_tnext_x1, I_tnext_x2, I_tnext_x3, I_tnext_x4]\n",
    "#         X_tnext = [I_tnext_x1, I_tnext_x2, I_tnext_x3]\n",
    "#         X_tnext = [I_tnext_x1, I_tnext_x2]\n",
    "\n",
    "        ### Image t -> t+1\n",
    "        flow4 = self.fea4(I_t_x4, I_tnext_x4)\n",
    "        upflow4 = self.deconv4(flow4)\n",
    "        \n",
    "        I_tnext_x3_hat = warp(I_t_x3, upflow4*0.625)\n",
    "        flow3 = self.fea3(I_tnext_x3_hat, I_tnext_x3)\n",
    "        flow3 = flow3 + upflow4\n",
    "        upflow3 = self.deconv3(flow3)\n",
    "        \n",
    "        I_tnext_x2_hat = warp(I_t_x2, upflow3*1.25)\n",
    "        \n",
    "        flow2 = self.fea2(I_tnext_x2_hat, I_tnext_x2)\n",
    "        flow2 = flow2 + upflow3\n",
    "        upflow2 = self.deconv2(flow2)\n",
    "        \n",
    "        I_tnext_x1_hat = warp(I_t_x1, upflow2*1.5)\n",
    "        \n",
    "        flow1 = self.fea1(I_tnext_x1_hat, I_tnext_x1)\n",
    "        flow1 = flow1 + upflow2\n",
    "        upflow1 = self.deconv1(flow1)\n",
    "        \n",
    "        I_tnext_hat = warp(I_t, upflow1*5.0)\n",
    "        \n",
    "        X_hat_tnext = [I_tnext_x1_hat, I_tnext_x2_hat, I_tnext_x3_hat]\n",
    "#         X_hat_tnext = [I_tnext_x1_hat, I_tnext_x2_hat]\n",
    "#         X_hat_tnext = [I_tnext_x1_hat]\n",
    "        f_t_tnext = [flow1, flow2, flow3, flow4]\n",
    "#         f_t_tnext = [flow1, flow2, flow3]\n",
    "#         f_t_tnext = [flow1, flow2]\n",
    "        \n",
    "        ### Image t+1 -> t\n",
    "        rev_flow4 = self.fea4(I_tnext_x4, I_t_x4)\n",
    "        rev_upflow4 = self.deconv4(rev_flow4)\n",
    "        \n",
    "        I_t_x3_hat = warp(I_tnext_x3, rev_upflow4*0.625)\n",
    "        \n",
    "        rev_flow3 = self.fea3(I_t_x3_hat, I_t_x3)\n",
    "        rev_flow3 = rev_flow3 + rev_upflow4\n",
    "        rev_upflow3 = self.deconv3(rev_flow3)\n",
    "        \n",
    "        I_t_x2_hat = warp(I_tnext_x2, rev_upflow3*1.25)\n",
    "\n",
    "        rev_flow2 = self.fea2(I_t_x2_hat, I_t_x2)\n",
    "        rev_flow2 = rev_flow2 + rev_upflow3\n",
    "        rev_upflow2 = self.deconv2(rev_flow2)\n",
    "        \n",
    "        I_t_x1_hat = warp(I_tnext_x1, rev_upflow2*2.5)\n",
    "        \n",
    "        rev_flow1 = self.fea1(I_t_x1_hat, I_t_x1)\n",
    "        rev_flow1 = rev_flow1 + rev_upflow2\n",
    "        rev_upflow1 = self.deconv1(rev_flow1)\n",
    "        \n",
    "        I_t_hat = warp(I_tnext, rev_upflow1*5.0)\n",
    "        \n",
    "        X_hat_t = [I_t_x1_hat, I_t_x2_hat, I_t_x3_hat]\n",
    "        f_tnext_t = [rev_flow1, rev_flow2, rev_flow3, rev_flow4]\n",
    "        \n",
    "#         X_hat_t = [I_t_x1_hat, I_t_x2_hat]\n",
    "#         f_tnext_t = [rev_flow1, rev_flow2, rev_flow3]\n",
    "\n",
    "#         X_hat_t = [I_t_x1_hat]\n",
    "#         f_tnext_t = [rev_flow1, rev_flow2]\n",
    "        \n",
    "        return X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext, f_tnext_t, I_t_hat, I_tnext_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.zeros([1, 3, 160, 240]).to(device)\n",
    "# b = torch.zeros([1, 3, 160, 240]).to(device)\n",
    "\n",
    "# mcmodel = MCJepa(3).to(device)\n",
    "# mcmodel(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AR8B0FDnz-Lq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def off_diagonal(M):\n",
    "    res = M.clone()\n",
    "    res.diagonal(dim1=-1, dim2=-2).zero_()\n",
    "    return res\n",
    "\n",
    "def vc_reg(X_t, X_tnext, lm, mu, nu):\n",
    "    N = X_t.shape[0]\n",
    "    C = X_t.shape[1]\n",
    "    H = X_t.shape[2]\n",
    "    W = X_t.shape[3] \n",
    "    D = C + H + W \n",
    "    mse_loss = nn.MSELoss()\n",
    "    sim_loss = mse_loss(X_t, X_tnext)\n",
    "    \n",
    "    std_z_a = torch.sqrt(X_t.var(dim=0) + 1e-04)\n",
    "    std_z_b = torch.sqrt(X_tnext.var(dim=0) + 1e-04)\n",
    "    std_loss = torch.mean(F.relu(1-std_z_a)) + torch.mean(F.relu(1-std_z_b))\n",
    "\n",
    "    X_t = X_t - X_t.mean(dim=0)\n",
    "    X_tnext = X_tnext - X_tnext.mean(dim=0)\n",
    "    cov_z_a = torch.matmul(X_t.view(N, C, W, H), X_t)/ (N-1)\n",
    "    cov_z_b = torch.matmul(X_tnext.view(N, C, W, H), X_tnext)/ (N-1)\n",
    "    conv_loss = (off_diagonal(cov_z_a).pow_(2).sum()/D) + (off_diagonal(cov_z_b).pow_(2).sum()/D) \n",
    "        \n",
    "    loss = lm*sim_loss + mu*std_loss + nu*conv_loss\n",
    "    return loss\n",
    "\n",
    "def cycle_loss_fn(f_t_tnext, f_tnext_t, X_t, X_tnext, lambda_a, lambda_b):\n",
    "    loss_cycle_A = torch.tensor(0.0).to(device)\n",
    "    loss_cycle_B = torch.tensor(0.0).to(device)\n",
    "    for i in range(1, len(X_t)):\n",
    "        loss_cycle_A += F.l1_loss(warp(X_t[i], f_t_tnext[i]), X_tnext[i]) * lambda_a\n",
    "        loss_cycle_B += F.l1_loss(warp(X_tnext[i], f_tnext_t[i]), X_t[i]) * lambda_b\n",
    "        \n",
    "    return loss_cycle_A + loss_cycle_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ph4H5n2d55WX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MCJepa_criterion(X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext, f_tnext_t, I_hat_t, I_hat_tnext, img1, img2, lm, mu, nu, lambda_a, lambda_b,show=False):\n",
    "  \n",
    "    mse_loss = nn.MSELoss()    \n",
    "    rec_loss = nn.MSELoss()\n",
    "    reg_loss = nn.MSELoss()\n",
    "    \n",
    "    regress_loss_forward = torch.tensor(0.0).to(device)\n",
    "    regress_loss_backward = torch.tensor(0.0).to(device)\n",
    "    for i in range(len(X_hat_tnext)):\n",
    "        regress_loss_forward += reg_loss(X_hat_tnext[i], X_tnext[i])\n",
    "        regress_loss_backward += reg_loss(X_hat_t[i], X_t[i])\n",
    "\n",
    "    reconst_loss_forward = rec_loss(I_hat_tnext, img2)\n",
    "    reconst_loss_backward = rec_loss(I_hat_t, img1)\n",
    "    vc_reg_loss = torch.tensor(0.0).to(device)\n",
    "    \n",
    "    for i in range(len(X_t)):\n",
    "        vc_reg_loss += vc_reg(X_t[i], X_tnext[i], lm, mu, nu)\n",
    "\n",
    "    cycle_loss = cycle_loss_fn(f_t_tnext, f_tnext_t, X_t, X_tnext, lambda_a, lambda_b)\n",
    "    \n",
    "    if show:\n",
    "        print(\"regress_loss_forward: \",50*regress_loss_forward)\n",
    "        print(\"regress_loss_backward: \",50*regress_loss_backward)\n",
    "        print(\"reconst_loss_forward: \",1000*reconst_loss_forward)\n",
    "        print(\"reconst_loss_backward: \",1000*reconst_loss_backward)\n",
    "        print(\"vc_reg_loss: \",vc_reg_loss)\n",
    "        print(\"cycle_loss: \",50*cycle_loss)\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "    return 50*regress_loss_forward + 50*regress_loss_backward + 1000*reconst_loss_forward + 1000*reconst_loss_backward + 50*cycle_loss + vc_reg_loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2rN0NwpeTrJG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_MCJepa(model, epochs, dataloader, criterion, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model = model.state_dict()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0.0\n",
    "        \n",
    "        pbar = tqdm(dataloader, leave=False)\n",
    "\n",
    "        for j,batch in enumerate(pbar):\n",
    "            if j == 4333: \n",
    "                break\n",
    "\n",
    "            frame_list = batch\n",
    "            total_train_loss = 0.0\n",
    "            \n",
    "            for i in range(len(frame_list) - 1):\n",
    "                img1 = frame_list[i].to(device)\n",
    "                img2 = frame_list[i+1].to(device)\n",
    "#                 print(img1.shape)\n",
    "                X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "                f_tnext_t, I_hat_t, I_hat_tnext = model(img1, img2)\n",
    "\n",
    "                loss = criterion(X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "                                f_tnext_t, I_hat_t, I_hat_tnext, img1, img2,\\\n",
    "                                lm, mu, nu, lambda_a, lambda_b,(i+21*j)%660 == 0)\n",
    "            \n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 scheduler.step(e+j/len(dataloader))\n",
    "            \n",
    "            pbar.set_postfix({'Video Loss': total_train_loss/(len(frame_list)-1)})\n",
    "\n",
    "            if total_train_loss/(len(frame_list)-1) < best_loss:\n",
    "                best_loss = total_train_loss/(len(frame_list)-1)\n",
    "                best_model = model.state_dict()\n",
    "                \n",
    "            if j % 30 == 0 and j > 0:\n",
    "                torch.save(best_model,\"best_model.pth\")\n",
    "                pbar.set_postfix({'Video Loss': total_train_loss/(len(frame_list)-1), 'Saved model with loss': best_loss})\n",
    "            \n",
    "        pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1), 'Saved model at j': j})\n",
    "        torch.save(best_model, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to figure out later\n",
    "in_features = 3 \n",
    "lm, mu, nu, lambda_a, lambda_b = 0.02, 0.02, 0.01, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8rPjQs1Gh1-N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MCJepa_model = MCJepa(in_features).to(device)\n",
    "optimizer = optim.SGD(MCJepa_model.parameters(), lr = 10e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/834 [00:00<?, ?it/s]/home/pt2310/.local/lib/python3.10/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regress_loss_forward:  tensor(48.9951, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "regress_loss_backward:  tensor(52.0666, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "reconst_loss_forward:  tensor(22.3646, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "reconst_loss_backward:  tensor(21.9918, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "vc_reg_loss:  tensor(251.8696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "cycle_loss:  tensor(73.2756, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 31/834 [01:36<41:05,  3.07s/it, Video Loss=71.9, Saved model with loss=50.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regress_loss_forward:  tensor(0.5515, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "regress_loss_backward:  tensor(0.5765, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "reconst_loss_forward:  tensor(26.6051, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "reconst_loss_backward:  tensor(27.0318, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "vc_reg_loss:  tensor(0.2472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "cycle_loss:  tensor(9.2255, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_MCJepa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMCJepa_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMCJepa_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mtrain_MCJepa\u001b[0;34m(model, epochs, dataloader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m                 loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m                 \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#                 scheduler.step(e+j/len(dataloader))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: total_train_loss\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frame_list)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/sgd.py:283\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(params) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m grouped_tensors \u001b[38;5;241m=\u001b[39m \u001b[43m_group_tensors_by_device_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device_params, device_grads, device_momentum_buffer_list, indices \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    285\u001b[0m     device_has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(grad\u001b[38;5;241m.\u001b[39mis_sparse \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m device_grads)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_foreach_utils.py:36\u001b[0m, in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m     33\u001b[0m             per_device_and_dtype_tensors[key][j]\u001b[38;5;241m.\u001b[39mappend(tensorlistlist[j][i])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# tack on previous index\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mper_device_and_dtype_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_and_dtype_tensors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_MCJepa(MCJepa_model, 10, train_dataloader, MCJepa_criterion, optimizer) # Training the MC JEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxmI1b9wcl33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PATH = \"best_model.pth\"\n",
    "# MCJepa_model = MCJepa(in_features).to(device)\n",
    "# MCJepa_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flow_1 = 0\n",
    "# flow_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "# #     MCJepa_model.reset_flows()\n",
    "#     frame_list = batch\n",
    "#     img1 = frame_list[0].to(device)\n",
    "#     img2 = frame_list[1].to(device)\n",
    "\n",
    "#     X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "#                 f_tnext_t, I_hat_t, I_hat_tnext = MCJepa_model(img1, img2)\n",
    "\n",
    "#     loss = MCJepa_criterion(X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "#                     f_tnext_t, I_hat_t, I_hat_tnext, img1, img2,\\\n",
    "#                      lm, mu, nu, lambda_a, lambda_b)\n",
    "    \n",
    "#     print(\"Loss: \", loss.item())\n",
    "# #     flow_1 = f_t_tnext\n",
    "#     flow_2 = f_t_tnext\n",
    "# #     print(f_t_tnext[0])\n",
    "#     print((f_t_tnext[0] == 0).all())\n",
    "    \n",
    "\n",
    "#     show_normalized_image(img1[0])\n",
    "#     plt.show()\n",
    "#     show_normalized_image(I_hat_t[0])\n",
    "#     plt.show()\n",
    "#     show_normalized_image(img2[0])\n",
    "#     plt.show()\n",
    "#     show_normalized_image(I_hat_tnext[0])\n",
    "#     plt.show()\n",
    "#     show_normalized_image(torch.square(I_hat_t[0]-img2[0]))\n",
    "#     plt.show()\n",
    "#     show_normalized_image(torch.square(I_hat_tnext[0]-img1[0]))\n",
    "#     plt.show()\n",
    "#     show_normalized_image(torch.square(img2[0]-img1[0]))\n",
    "    \n",
    "# #     print(Y1)\n",
    "#     break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for param in MCJepa_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class DoubleConv(nn.Module):\n",
    "#     \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if not mid_channels:\n",
    "#             mid_channels = (out_channels + in_channels)//2\n",
    "            \n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(mid_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "\n",
    "# class Down(nn.Module):\n",
    "#     \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.maxpool_conv = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(in_channels, out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "# class Up(nn.Module):\n",
    "#     \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "#         if bilinear:\n",
    "#             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "#             self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "#         else:\n",
    "#             self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "#             self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         x1 = self.up(x1)\n",
    "#         # input is CHW\n",
    "#         diffY = x2.size()[2] - x1.size()[2]\n",
    "#         diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    " \n",
    "#         x = torch.cat([x2, x1], dim=1)\n",
    "#         return self.conv(x)\n",
    "\n",
    "\n",
    "# class OutConv(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(OutConv, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x)\n",
    "    \n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_channels = 3, n_classes = 49, bilinear=False):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "#         self.bilinear = bilinear\n",
    "\n",
    "#         self.inc = (DoubleConv(n_channels, 64))\n",
    "#         self.down1 = (Down(64, 128))\n",
    "#         self.down2 = (Down(128, 256))\n",
    "#         self.down3 = (Down(256, 512))\n",
    "#         factor = 2 if bilinear else 1\n",
    "#         self.down4 = (Down(512, 1024 // factor))\n",
    "#         self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "#         self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "#         self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "#         self.up4 = (Up(128, 64, bilinear))\n",
    "#         self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1 = self.inc(x)\n",
    "#         x2 = self.down1(x1)\n",
    "#         x3 = self.down2(x2)\n",
    "#         x4 = self.down3(x3)\n",
    "#         x5 = self.down4(x4)\n",
    "#         x = self.up1(x5, x4)\n",
    "#         x = self.up2(x, x3)\n",
    "#         x = self.up3(x, x2)\n",
    "#         x = self.up4(x, x1)\n",
    "#         logits = self.outc(x)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjKcceQycosj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def train_fine_tune(downstream_model, epochs, dataloader, criterion, optimizer):\n",
    "#     downstream_model.train()\n",
    "\n",
    "#     train_losses = []\n",
    "\n",
    "#     best_loss = float(\"inf\")\n",
    "# #     best_model = {}\n",
    "#     best_model = downstream_model.state_dict()\n",
    "\n",
    "#     for _ in range(epochs):\n",
    "#         total_train_loss = 0.0\n",
    "#         total_train_correct = 0.0\n",
    "\n",
    "#         pbar = tqdm(dataloader, leave=False)\n",
    "\n",
    "#         for j, batch in enumerate(pbar):\n",
    "          \n",
    "#             if j == 333:\n",
    "#                 break\n",
    "                \n",
    "#             frame_list, mask_list = batch[0], batch[1] # TODO\n",
    "#             total_train_loss = 0.0\n",
    "            \n",
    "\n",
    "#             for i in range(len(frame_list) - 1):\n",
    "#                 img1 = frame_list[i].to(device)\n",
    "#                 img2 = frame_list[i+1].to(device)\n",
    "#                 mask_list = mask_list.type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "#                 logits = downstream_model(img1)\n",
    "#                 loss = criterion(logits, mask_list[:,i])\n",
    "\n",
    "#                 total_train_loss += loss.item()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1)})\n",
    "\n",
    "#             if total_train_loss/(len(frame_list)-1) < best_loss:\n",
    "#                 best_loss = total_train_loss/(len(frame_list)-1)\n",
    "#                 best_model = downstream_model.state_dict()\n",
    "\n",
    "#             if j%25 == 0:\n",
    "#                 torch.save(best_model,\"best_downstream_model.pth\")\n",
    "#                 pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1), 'Saved downstream model with loss': best_loss})\n",
    "          \n",
    "#         pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1), 'Saved model at j': j})\n",
    "#         torch.save(downstream_model.state_dict(), \"downstream_model.pth\")\n",
    "          \n",
    "#     torch.save(best_model,\"best_downstream_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "uWFHc_7-_Xel",
    "outputId": "0b16b015-60cd-4bdb-ae94-eb3e266eb04b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# in_features_downstream = 16\n",
    "\n",
    "# downstream_model = UNet().to(device)\n",
    "# downstream_optimizer = optim.RMSprop(downstream_model.parameters(),\n",
    "#                           lr=1e-5, weight_decay=1e-8, momentum=0.999, foreach=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_fine_tune(downstream_model, 5, downstream_dataloader, criterion, downstream_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJfqVQ0nM6wl",
    "outputId": "9a167640-cc43-4609-c286-92ec159d541c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PATH = \"best_downstream_model.pth\"\n",
    "# downstream_model = UNet().to(device)\n",
    "# downstream_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchmetrics\n",
    "# import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jaccard = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=49).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def test(downstream_model, JepaModel, epochs, dataloader, criterion):\n",
    "#     train_losses = []\n",
    "\n",
    "#     best_loss = float(\"inf\")\n",
    "#     best_model = downstream_model.state_dict()\n",
    "\n",
    "#     for _ in range(epochs):\n",
    "#         total_train_loss = 0.0\n",
    "\n",
    "#         pbar = tqdm(dataloader, leave=False)\n",
    "\n",
    "#         for j,batch in enumerate(pbar):\n",
    "          \n",
    "#             frame_list, mask_list = batch[0], batch[1]\n",
    "#             total_train_loss = 0.0\n",
    "            \n",
    "\n",
    "#             for i in range(len(frame_list) - 1):\n",
    "#                 img1 = frame_list[i].to(device)\n",
    "#                 img2 = frame_list[i+1].to(device)\n",
    "#                 mask_list = mask_list.type(torch.LongTensor).to(device)\n",
    "\n",
    "#                 mask_pred = downstream_model(img1)\n",
    "# #               \n",
    "#                 if i == 20:\n",
    "#                     print(mask_pred.shape, mask_list[:,i].shape)\n",
    "#                     print(mask_list[:,i][0])\n",
    "\n",
    "#                     print(jaccard(mask_pred, mask_list[:,i]))\n",
    "#                     print((torch.argmax(mask_pred[0], dim=0) == 0).all())\n",
    "#                     plt.imshow(mask_list[0][i].cpu())\n",
    "#                     plt.show()\n",
    "#                     plt.imshow(mask_pred.argmax(dim=1)[0].float().cpu())\n",
    "#                     plt.show()\n",
    "#                 loss = criterion(mask_pred, mask_list[:,i])\n",
    "#                 total_train_loss += loss.item()\n",
    "\n",
    "#             pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test(downstream_model, MCJepa_model, 1, val_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL7vGZCYM6wl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def real_test(downstream_model, JepaModel, epochs, dataloader, criterion, scale = 0.1):\n",
    "    \n",
    "\n",
    "#     train_losses = []\n",
    "\n",
    "#     best_loss = float(\"inf\")\n",
    "#     best_model = downstream_model.state_dict()\n",
    "\n",
    "#     for _ in range(epochs):\n",
    "#         total_train_loss = 0.0\n",
    "\n",
    "#         pbar = tqdm(dataloader, leave=False)\n",
    "        \n",
    "#         avg_jacc = 0.0\n",
    "\n",
    "#         for j,batch in enumerate(pbar):\n",
    "          \n",
    "#             frame_list, mask_list = batch[0], batch[1] # TODO\n",
    "#             total_train_loss = 0.0\n",
    "#             X_tconcat = None\n",
    "#             I_hat_t = None\n",
    "#             I_hat_tnext = None\n",
    "#             I_hat_t = None\n",
    "            \n",
    "#             final_flow = 0.0\n",
    "#             for i in range(11):\n",
    "#                 img1 = frame_list[i].to(device)\n",
    "#                 img2 = frame_list[i+1].to(device)\n",
    "#                 mask_list = mask_list.type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "#                 X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "#                 f_tnext_t, I_hat_t, I_hat_tnext = JepaModel(img1, img2)\n",
    "#                 final_flow = JepaModel.deconv1(f_t_tnext[0])*5.0 + scale*final_flow\n",
    "                \n",
    "            \n",
    "#             mask_pred = downstream_model(frame_list[11].to(device))\n",
    "# #             mask_pred_ = downstream_model(frame_list[11].to(device))\n",
    "            \n",
    "#             for i in range(11):\n",
    "#                 X_t, X_tnext, X_hat_t, X_hat_tnext, f_t_tnext,\\\n",
    "#                 f_tnext_t, I_hat_t, I_hat_tnext = JepaModel(I_hat_t, I_hat_tnext)\n",
    "# #                 print(f_t_tnext[0].shape, mask_pred.shape)\n",
    "#                 flow = JepaModel.deconv1(f_t_tnext[0]) \n",
    "#                 flow *= 0\n",
    "#                 mask_pred = warp(mask_pred, flow)\n",
    "\n",
    "# #             mask_pred = warp(mask_pred, final_flow)\n",
    "            \n",
    "# #             plt.imshow(unnormalize(img1[0]).permute(1, 2, 0).cpu())\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(unnormalize(I_hat_tnext[0]).permute(1, 2, 0).cpu())\n",
    "# #             plt.show()\n",
    "# #             mask_pred = downstream_model(I_hat_tnext)\n",
    "\n",
    "# #             print(f_t_tnext)\n",
    "# #             print(jaccard(mask_pred, mask_list[:,21]))\n",
    "# #             print((torch.argmax(mask_pred[0], dim=0) == torch.argmax(mask_pred_[0].cpu())).all())\n",
    "            \n",
    "# #             plt.imshow(mask_list[0][21].cpu())\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(torch.argmax(mask_pred[0].cpu(), dim=0))\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(torch.argmax(mask_pred_[0].cpu(), dim=0))\n",
    "# #             plt.show()\n",
    "#             loss = criterion(mask_pred, mask_list[:,21])\n",
    "#             total_train_loss += loss.item()\n",
    "            \n",
    "# #             plt.imshow(unnormalize(img1[0]).permute(1, 2, 0).cpu())\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(unnormalize(I_hat_tnext[0]).permute(1, 2, 0).cpu())\n",
    "# #             plt.show()\n",
    "# #             mask_pred = downstream_model(I_hat_tnext)\n",
    "\n",
    "# #             print(f_t_tnext)\n",
    "# #             print(jaccard(mask_pred, mask_list[:,21]))\n",
    "# #             print((torch.argmax(mask_pred[0], dim=0) == torch.argmax(mask_pred_[0].cpu())).all())\n",
    "            \n",
    "# #             plt.imshow(mask_list[0][21].cpu())\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(torch.argmax(mask_pred[0].cpu(), dim=0))\n",
    "# #             plt.show()\n",
    "# #             plt.imshow(torch.argmax(mask_pred_[0].cpu(), dim=0))\n",
    "# #             plt.show()\n",
    "\n",
    "#             avg_jacc += jaccard(mask_pred, mask_list[:,21])\n",
    "#             pbar.set_postfix({'Per frame Loss': total_train_loss/(len(frame_list)-1), 'avg_jaccard': avg_jacc.item() / (j+1)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# real_test(downstream_model, MCJepa_model, 1, val_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
